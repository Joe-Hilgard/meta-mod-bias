---
title: "Analysis_MetaMod_Markdown"
author: "Olivia Cody"
date: "4/12/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load packages and functions
library(MASS)
library(tidyverse)
library(truncnorm)
library(truncdist)
library(pwr)
library(compiler)
library(metafor)
library(forcats)
library(weightr)
source("sim-studies/sim-studies.R", chdir=TRUE)
#source("2_analysis.R")

# Load simulated data
load("sim.Rdata")
```
Todo:


- Writing: Please start writing up the results section -- use the plots as well as the data frames underlying the plots.

- For each bias scenario:
    for each estimator:
       consider ME, RMSE, Type I and Type II error.
- My advice: just talk about each metric (ME, RMSE, Type I, Power) in sequence, describing whether (1) there's a problem with the unadjusted estimator indicating that we need adjustments for pub bias, and (2) whether the adjustments make things any better, and if so, which ones work best.

It is widely understood that the selective publication of statistically significant results is hazardous to hypothesis testing, as it leads to overestimation of the mean effect size. Less understood, however, is whether selective publication influences estimates of meta-moderation, that is, study attributes associated with changes in the true effect size. Insofar as publication bias may influence moderator estimates, it is not clear how to correct those moderator estimates. Although there are a number of statistical procedures and models that attempt to adjust for publication bias, it is not clear how well moderator parameters are recovered following these adjustments.

To address this question, we simulate research literatures in which the true effect varies across studies as a function of some study-level moderator. We then simulate different degrees of selection bias and p-hacking. Following this, we observe how estimates of the moderator parameters have been altered by selection bias and p-hacking. Finally, we evaluate a variety of adjustments for publication bias for their ability to recover the moderation parameter.

# What is meta-moderation
# What is publication bias
# What adjustments are available
A number of adjustments for publication bias exist. However, not all of these are compatible with moderation analysis, and among those that are, their performance has not been thoroughly tested.

Selection models attempt to estimate the true effect through modeling the probability of publication of non-significant results (Hedges & Vevea? Vevea & Hedges?). An R package to fit these models is available, and one can specify moderators in this analysis. One simply specifies a "mods" argument in the weightr::weightfunct() function (Coburn & Vevea, 2017). 
P-curve and p-uniform are simpler selection models. They assume that non-significant results are never published, and if any are published, they discard them anyway. These techniques, while popular in social psychology, do not allow for moderation analysis. 

Trim and fill is a popular adjustment for publication bias that attempts to impute missing studies through inspection of the funnel plot. Although it is popular in psychology and easy to use in point-and-click meta-analysis software, it is not very effective in adjusting for moderate to serious bias (Carter et al. ...). Additionally, trim and fill does not allow for moderation analysis.

PET meta-regression attempts to partial out the effect of publication bias as a small-study effect. It fits a regression to the relationship between effect size and the standard error. The intercept, representing a hypothetical study with a standard error of zero, is interpreted as the bias-adjusted effect size.
This approach has the weakness of underestimating true effects.

PEESE meta-regression similarly attempts to partial out the effect of publication bias as a small-study effect, but instead of the standard error, it uses the variance. This models publication bias as a quadratic effect, stronger among small studies and weaker among large studies. 
This approach has the weakness of overestimating null effects.

Stanley and Doucouliagos (2014, equation 10) suggest that PET-PEESE meta-regression can be applied in a multiple regression format. They suggest a multiple regression model in which the standard error or variance and the moderator are both predictors of the effect size. This approach simultaneously models publication bias (via PET or PEESE, depending on the use of standard error or variance) and the moderator.

It is still unclear how best to apply this approach, or how effective it is. One has to choose between PET and PEESE in the moderation model; no guideline has been offered. In offering this model, Stanley and Doucouliagos do not use Monte Carlo simulation to evaluate its efficacy. This multiple regression approach may be effective, or it may have some unacceptable bias or variability in practical applications.

One limitation of this approach might be that one fits a single small-study effect across all groups in the moderation analysis. This may be deleterious if some groups have no true effect and others have some true effect; they would be subject to different amounts of publication bias, and so may have different slopes. In this simulation, we try it both ways, fitting a single small-study slope across all groups and modeling a small-study slope that varies by group.

# Method
Monte Carlo simulation was used to test the bias and efficiency of moderator analysis across biasing scenarios and adjustments for bias. 

Data were simulated according to two scenarios. In the "medium effects" scenario, studies had true effects of 0, 0.3, or 0.6, depending on their level of the moderator. In the "small effects" scenario, studies had true effects of 0, 0.2, or 0.4, depending on their level of the moderator. 

Two levels of bias were implemented. In one condition, there was no publication bias or p-hacking. In the other, a nonsignificant result had only a 20% chance of publication. Additionally, 50% of studies involved aggressive p-hacking, and a further 40% used moderate p-hacking.
[TODO: define these p-hacking algorithms]
Studies were simulated until each moderator level had 20 studies. This avoids confounding the effects of publication bias with the effects of reduced number of studies.

Studies were simulated with a sample size of minimum 20/cell, maximum 200/cell, average 50/cell.
##**Analysis**

Na√Øve analysis: Meta-analysis tests the moderator effect without adjustment
for publication bias

WLS-MRA (single slope): Meta-analysis tests the moderator effect after PET
adjustment for publication bias. PET slope is fixed across groups.

WLS-MRA (variable slope): Meta-analysis tests the moderator effect after
PET adjustment for publication bias. PET slope may vary across groups.
Results are presented in terms of bias, root mean squared error, and Type I/II
error.


```{r, echo=FALSE, include=FALSE}
# summarize results of set 1 and set 2
summarize_run(res.nobias) 
# estimates: d = .00, .30, .60; group 1 Type I, 5%; Moderator power: 100%, 100%; 
# TODO: Get Type I error / power of Egger test
# PET shows not-so-bad downward bias, d = .26. Wasn't it worse before?
# check out the super poor power on the interactive model!

summarize_run(res.medPB)
# estimates: d = .13, .40, .63; group 1 Type I, 81%; Moderator power: 100%, 100%;
# note that differences between subpopulations have been halved
# PET bias is really bad, probably drug down by the bad subgroup
# interactive model makes it worse, not better.

summarize_run(res.smallfx.nobias) 
# observed d = 0, .2, .4; power = 9%, 87%, 100%; egger type 1 ??%
summarize_run(res.smallfx.medPB) # 
# observed d = .14, .33, .47; Type I = 83%; power = 89%, 100%; 2.2%, 35%; egger power ?%

plotCellMeans(res.nobias, res.medPB, "No Bias", "Med Bias")
plotParamMeans(res.nobias, res.medPB, "No Bias", "Med Bias")
plotCellMeans(res.smallfx.nobias, res.smallfx.medPB, "No Bias", "Med Bias")
plotParamMeans(res.smallfx.nobias, res.smallfx.medPB, "No Bias", "Med Bias")

summarize_run(res.medPB) 
# cell means: .35, .44, .64
# Additive PET-RMA has downward bias: .00, .11, .29
# Interactive PET-RMA has downward bias, but less bad: -.10, .09, .44
# moderator power: 34%, 100%
# PET-RMA seems to make moderator power worse, not better.

summarize_run(res.hiPB)
plotCellMeans(res.nobias, res.medPB, "No Bias", "Hi Bias")
plotParamMeans(res.nobias, res.medPB, "No Bias", "Hi Bias")
# Not that bad, but these are big differences

summarize_run(res.smallfx.hiPB)
plotCellMeans(res.smallfx.nobias, res.smallfx.medPB, "No Bias", "Hi Bias")
plotParamMeans(res.smallfx.nobias, res.smallfx.medPB, "No Bias", "Hi Bias")
# power drops to 47%, 97% from 87%, 100%
# PET-RMA seems to improve power slightly (additive) but terrible in interactive

# Story not quite so damning as I'd thought if we conduct the simulations this way
# QRPs may change the story but I'm not sure if I've got the cycles
# Check http://shinyapps.org/apps/metaExplorer/ to see if things look like real-life metas
# I might say it does. Although maybe med + high QRP is more like it?
# RE with med bias, no QRP turns delta = 0, 0.2, 0.5 to d = .15, .32, .55
# RE with med bias, high QRP turns delta = 0, 0.2, 0.5 to d = .28, .38, .57
# RE with hi bias, no QRP turns delta = 0, 0.2, 0.5 to d = .3, .38, .56
# RE with hi bias hi QRP turns delta = 0, 0.2, 0.5 to d = .39, .41, .57
# It doesn't seem to matter a whole lot for the naive estimator. but QRP will mess up PET.

summarize_run(res.hiPB)
summarize_run(res.smallfx.hiPB)
summarize_run(res.medPB.hiQRP)
summarize_run(res.smallfx.nobias)
summarize_run(res.smallfx.medPB.hiQRP)
summarize_run(res.smallfx.hiPB)

# Next steps:
# increase nSim
# inspect intermediate steps for quality, e.g. unit tests
# explore different values of vector d
# check distribution of N -- is it appropriate?
#hist(meta1$N)
# hist(meta1$N)

# implement PET-RMA model
delta <- c(0, .3, .6)
# This function goes through all the estimators and 
# Calculates their performance (ME, RMSE, TypeI/TypeII)
mySummary <- function(x) {
  x %>% 
    summarize(
      # Mean error
      me.b1 = mean(mod.obs.b1 - delta[1]),
      me.b2 = mean(mod.obs.b2 - delta[2]),
      me.b3 = mean(mod.obs.b3 - delta[3]),
      me.b1.add = mean(joint.add.b1 - delta[1]),
      me.b2.add = mean(joint.add.b2 - delta[2]),
      me.b3.add = mean(joint.add.b3 - delta[3]),
      me.b1.inter = mean(joint.inter.b1 - delta[1]),
      me.b2.inter = mean(joint.inter.b2 - delta[2]),
      me.b3.inter = mean(joint.inter.b3 - delta[3]),
      me.b1.sel = mean(sel.b1 - delta[1]),
      me.b2.sel = mean(sel.b2 - delta[2]),
      me.b3.sel = mean(sel.b3 - delta[3]),
      # RMSE        
      rmse.b1 = sqrt(mean((mod.obs.b1 - delta[1])^2)),
      rmse.b2 = sqrt(mean((mod.obs.b2 - delta[2])^2)),
      rmse.b3 = sqrt(mean((mod.obs.b3 - delta[3])^2)),
      rmse.b1.add = sqrt(mean((joint.add.b1 - delta[1])^2)),
      rmse.b2.add = sqrt(mean((joint.add.b2 - delta[2])^2)),
      rmse.b3.add = sqrt(mean((joint.add.b3 - delta[3])^2)),
      rmse.b1.inter = sqrt(mean((joint.inter.b1 - delta[1])^2)),
      rmse.b2.inter = sqrt(mean((joint.inter.b2 - delta[2])^2)),
      rmse.b3.inter = sqrt(mean((joint.inter.b3 - delta[3])^2)),
      rmse.b1.sel = sqrt(mean((sel.b1 - delta[1])^2)),
      rmse.b2.sel = sqrt(mean((sel.b2 - delta[2])^2)),
      rmse.b3.sel = sqrt(mean((sel.b3 - delta[3])^2)),
      # Power / Type I  
      pow.b1 = mean(mod.p1 < .05),
      pow.b2 = mean(mod.p2 < .05),
      pow.b3 = mean(mod.p3 < .05),
      pow.b1.add = mean(joint.add.p1 < .05),
      pow.b2.add = mean(joint.add.p2 < .05),
      pow.b3.add = mean(joint.add.p3 < .05),
      pow.b1.inter = mean(joint.inter.p1 < .05),
      pow.b2.inter = mean(joint.inter.p2 < .05),
      pow.b3.inter = mean(joint.inter.p3 < .05),
      pow.b1.sel = mean(sel.p1 < .05),
      pow.b2.sel = mean(sel.p2 < .05),
      pow.b3.sel = mean(sel.p3 < .05)
    )
}

# Make a final summary table and write it to CSV
final <- bind_rows(medFX_noBias = mySummary(res.nobias),
          #medFX_medPBhiQRP = mySummary(res.medPB.hiQRP),
          smallFX_noBias = mySummary(res.smallfx.nobias),
          smallFX_medPBhiQRP = mySummary(res.smallfx.medPB.hiQRP),
          .id = "id")
write.csv(final, "final_output.csv", row.names = F)


# TODO: DOCUMENT THIS. Make temp objects for plotting??
temp <- bind_rows(noBias = res.nobias, 
                  bias = res.medPB.hiQRP, 
                  .id = "id") %>% 
  select(id,
         mod.obs.b1:mod.obs.b3, 
         joint.add.b1:joint.add.b3, 
         joint.inter.b1:joint.inter.b3,
         sel.b1:sel.b3) %>% 
  gather(key = "key", value = "value", mod.obs.b1:sel.b3) %>% 
  separate(key, into = c("junk", "estimator", "coefficient"), sep = "\\.",
           fill = "left") %>% 
  # augment with true values & fix factor order
  mutate(delta = ifelse(coefficient == "b1", 0,
                        ifelse(coefficient == "b2", 0.3, 0.6)),
         rmse = sqrt((value - delta)^2),
         id = fct_relevel(id, c("noBias", "bias")),
         estimator = fct_relevel(estimator, c("obs", "add", "inter", "sel")))

temp2 <- bind_rows(noBias = res.smallfx.nobias, 
                  bias = res.smallfx.medPB.hiQRP, 
                  .id = "id") %>% 
  select(id,
         mod.obs.b1:mod.obs.b3, 
         joint.add.b1:joint.add.b3, 
         joint.inter.b1:joint.inter.b3,
         sel.b1:sel.b3) %>% 
  gather(key = "key", value = "value", mod.obs.b1:sel.b3) %>% 
  separate(key, into = c("junk", "estimator", "coefficient"), sep = "\\.",
           fill = "left") %>% 
  # augment with true values
  mutate(delta = ifelse(coefficient == "b1", 0,
                        ifelse(coefficient == "b2", 0.2, 0.4)),
         rmse = sqrt((value - delta)^2),
         id = fct_relevel(id, c("noBias", "bias")),
         estimator = fct_relevel(estimator, c("obs", "add", "inter", "sel")))
```

##**Results**

####**Medium effects**

**Group Means: 0, 0.3, 0.6**

Publication bias causes overestimation of Œ¥ = 0 and underestimation of mods.
This underestimation causes increased Type II error rates for moderators.
PET-RMA does not quite undo this bias and can have poor power and efficiency.

**Mean Estimates**

```{r, echo=FALSE}
# plot means
#Medium effects
temp %>% 
  group_by(id, estimator, coefficient, delta) %>% 
  summarize(m = mean(value),
            q.lower = quantile(value, .025),
            q.upper = quantile(value, .975)) %>% 
  ggplot(aes(x = coefficient, y = m, shape = estimator)) +
  geom_hline(aes(yintercept = delta), col = 'grey25') +
  geom_pointrange(aes(ymin = q.lower, ymax = q.upper),
                  size = .5, position = position_dodge(width = .5)) +
  facet_grid(~id) +
  scale_shape_manual(values = c(19, 17, 15, 18, 18, 18)) +
  scale_y_continuous("Estimate (d)",
                     breaks = c(-0.3, 0, 0.3, 0.6, 0.9)) 
ggsave("me_bigfx.png", width = 5, height = 4)
```

**Root Mean Squared Error**

```{r, echo=FALSE}
# plot RMSE
#Medium effects
temp %>%
group_by(id, estimator, coefficient) %>%
summarize(m = mean(rmse, na.rm=T),
          q.lower = quantile(rmse, .025),
          q.upper = quantile(rmse, .975)) %>%
ggplot(aes(x = coefficient, y = m, shape = estimator,
           ymin = q.lower, ymax = q.upper)) +
geom_pointrange(size= .5, position = position_dodge(width = .5)) +
facet_wrap(~id) +
scale_y_continuous("RMSE") +
ggsave("rmse_bigfx.png", width = 10.25, height = 4)

```

**Power/ Type I**

```{r, echo=FALSE}
# Power / Type I
#Meduim effects

#estimator on a facet_wrap or facet_grid

temp.p <- 
  # make power/type I dataset comparing nobias & mediumbias + hiQRP
  bind_rows(noBias = res.nobias, 
                    bias = res.medPB.hiQRP, 
                    .id = "id") %>% 
  select(id,
         mod.p1:mod.p3, 
         joint.add.p1:joint.add.p3, 
         joint.inter.p1:joint.inter.p3,
         sel.p1:sel.p3) %>% 
  gather(key = "key", value = "value", mod.p1:joint.inter.p3, sel.p1:sel.p3) %>% 
  separate(key, into = c("junk", "estimator", "coefficient"), sep = "\\.",
           # need fill = "left" to deal with shorter "mod.p1" vs "joint.add.p1"
           fill = "left") %>% 
  mutate(id = fct_relevel(id, "noBias", "bias"),
         estimator = fct_relevel(estimator, c("mod", "add", "inter")))
temp.p %>% 
  group_by(id, estimator, coefficient) %>% 
  summarize(sig = mean(value < .05, na.rm = T)) %>% 
  ggplot(aes(x = coefficient, y = sig, 
             shape = interaction(estimator, id),
             group = estimator)) +
  geom_point(size = 3, position = position_dodge(width = .5)) +
  scale_shape_manual(values = c(19, 17, 15, 18, 1, 2, 0, 5)) +
  facet_wrap(~id)
ggsave("nhst_bigfx.png", width = 6, height = 4)
```

**Significance Rates**

```{r, echo=FALSE}
#Significance Rates
#medium effect
temp.p %>% 
  group_by(id, estimator, coefficient) %>% 
  summarize(sig = mean(value < .05, na.rm = T)) %>% 
  ggplot(aes(x = coefficient, y = sig, 
             shape = estimator,
             group = estimator)) +
  geom_point(size = 3, position = position_dodge(width = .8)) +
  scale_shape_manual(values = c(19, 17, 15, 18, 1, 2, 0, 5)) +
  facet_wrap(~id)
ggsave("nhst_bigfx.png", width = 5, height = 4)
```


####**Small Effects**

**Group means: 0, 0.2, 0.4**

When effects are smaller, the loss of power is more pronounced.
Again, PET-RMA does not fix the biases and can inflict further loss of efficiency.

**Mean Estimates**

```{r, echo=FALSE}
#Plot means
#Small effects
temp2 %>% 
  group_by(id, estimator, coefficient, delta) %>% 
  summarize(m = mean(value, na.rm = T),
            q.lower = quantile(value, .025),
            q.upper = quantile(value, .975)) %>% 
  ggplot(aes(x = coefficient, y = m, shape = estimator)) +
  geom_hline(aes(yintercept = delta), col = 'grey25') +
  geom_pointrange(aes(ymin = q.lower, ymax = q.upper),
                  size = .5, position = position_dodge(width = .5)) +
  facet_grid(~id) +
  scale_shape_manual(values = c(19, 17, 15, 18, 18, 18)) +
  scale_y_continuous("Estimate (d)",
                     breaks = c(-0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8))
ggsave("me_smfx.png", width =5, height = 4)

```

**Root Mean Squared Error**

```{r, echo=FALSE}
#plot RMSE
#Small effects
temp2 %>%
  group_by(id, estimator, coefficient) %>%
  summarize(m = mean(rmse),
            q.lower = quantile(rmse, .025),
            q.upper = quantile(rmse, .975)) %>%
  ggplot(aes(x = coefficient, y = m, shape = estimator,
             ymin = q.lower, ymax = q.upper)) +
  geom_pointrange(size = .5, position = position_dodge(width = .5)) +
  facet_wrap(~id) +
  scale_y_continuous("RMSE") +
ggsave("rmse_smfx.png", width = 10.25, height = 4)
```


**Power/ Type I**

```{r, echo=FALSE}
# Power / Type I
#Small effects
temp.p2 <- bind_rows(noBias = res.smallfx.nobias, 
                    bias = res.smallfx.medPB.hiQRP, 
                    .id = "id") %>% 
  select(id,
         mod.p1:mod.p3, 
         joint.add.p1:joint.add.p3, 
         joint.inter.p1:joint.inter.p3,
         sel.p1:sel.p3) %>% 
  gather(key = "key", value = "value", mod.p1:sel.p3) %>% 
  separate(key, into = c("junk", "estimator", "coefficient"), sep = "\\.",
           # need fill = "left" to deal with shorter "mod.p1" vs "joint.add.p1"
           fill = "left") %>% 
  mutate(id = fct_relevel(id, "noBias", "bias"),
         estimator = fct_relevel(estimator, c("mod", "add", "inter")))
temp.p2 %>% 
  group_by(id, estimator, coefficient) %>% 
  summarize(sig = mean(value < .05, na.rm = T)) %>% 
  ggplot(aes(x = coefficient, y = sig, 
             shape = interaction(estimator, id),
             group = estimator)) +
  geom_point(size = 3, position = position_dodge(width = .5)) +
  scale_shape_manual(values = c(19, 17, 15, 18, 1, 2, 0, 5)) +
  facet_wrap(~id)
ggsave("nhst_smfx.png", width = 6, height = 4)
```

**Significance Rates**

```{r, echo=FALSE}
#Significance Rates
#Small effect
temp.p2 %>% 
  group_by(id, estimator, coefficient) %>% 
  summarize(sig = mean(value < .05, na.rm = T)) %>% 
  ggplot(aes(x = coefficient, y = sig, 
              shape = estimator,
             group = estimator)) +
  geom_point(size = 3, position = position_dodge(width = .8)) +
  scale_shape_manual(values = c(19, 17, 15, 18, 1, 2, 0, 5)) +
  facet_wrap(~id)
ggsave("nhst_smfx.png", width = 5, height = 4)
```

####**No publication bias**

**Type I error rate.** 
Under the null, all methods had appropriate Type I error rates, with no large differences between different methods. 


**Power.** 
For the medium effects, all models except the inter model had sufficient power. for an effect size of 0.3, the inter model only had power of around 20%. For an effect size of 0.6, the inter model had power of around 60%. Under small effects, the inter model preformed even worse, with a power of around 12% for a 0.2 effect size and around 35% for an effect size of 0.4. 


**ME.** 
All methods were generally unbiased. For medium effects, only for a true effect size of 0.6 was there any noticeable difference, here the inter model underestimated the true effect. Under the small effects, both the add and inter models underestimated a null effect. 

In terms of 95 percentile range, all of the methods preformed relatively equally except for the inter model. The inter model had a large standard error across all effect sizes. 

**RMSE.** 
The obs and sel models had the lowest RMSE across all effect sizes. The add model preformed slightly worse when the null was true but not for effect sizes above 0.2. Overall, the inter model has the highest RMSE with the largest 95 percentile range as well. 


**95% CI coverage.** 



####**Strong publication bias**

**Type I error rates.** Under the null, only the sel model performed well. All other models had a type 1 error rate higher than 0.10. 


**Power.** Across all effect sizes, all models except for the inter model preformed relatively equally with acceptable power (above 80%). Only under an effect size of 0.2 did the power drop for all models. Except for the inter model, the models had power of around 60% for an effect size of 0.2. Here, the add model appeared to have the most power. 

**ME.** 
Under the null, all methods performed poorly except the sel model which estimated the null correctly and had the smallest 95 percentile range. The obs model overestimated the effect size to be around 0.25 and both the add and inter models underestimated the null effect. Once again the inter model had the largest 95 percentile range.

For a medium effect of 0.3, all models under estimated the effect except for the inter model which overestimated the effect. For a medium effect of 0.6 the sel model preformed the best, only sightly underestimating the model. The obs and add model both underestimated the effect and the inter overestimated the effect. 

For a small effect of 0.2, again, all models underestimated the effect except for the inter model which overestimated the effect. For a small effect of 0.4 the sel model preformed the best, only slightly underestimating the model. The obs and add model both underestimated the effect and the inter overestimated the effect.

**RMSE.** 
Under the null, the sel model had the lowest RSME followed by the add model and the obs and inter models preforming the worst. 

Across all of the effect sizes the sel model had the lowest RMSE. The obs and add models steadily had increasing RMSE as the true effect increased. The inter model's RMSE slightly improved as the effect size increased relative to the add and obs models; however, the inter model was never as low as the sel. 

In terms of 95 percentile range, all models had relatively equal 95 percentile range across effect sizes with the exception of the inter model which had very large 95 percentile range across all effect sizes. 

**95% CI coverage.** 

##**Conclusion**

The sel model appears to perform the best across all scenarios and the inter model preforms the worst by far.
 